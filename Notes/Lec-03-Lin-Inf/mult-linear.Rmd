---
title: "Multiple Linear Regression and Inferences on Regression"
author: "Adam J Sullivan, PhD"
date: "1/29/2018"
output:
  ioslides_presentation: 
    widescreen: true
  beamer_presentation: default
notes: ''
link: yes
reading:  <a href="https://php-1511-2511.github.io/Introduction-to-R/">Introduction to R </a>
slides: yes
layout: page
css:  "C:/Users/adam_/Dropbox (Personal)/Brown/Teaching/Brown Courses/PHP2511/Spring 2018/website/php-1511-2511.github.io/knitr/slides.css"
---


```{r setup, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(error = TRUE)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_chunk$set(results="hold")
knitr::opts_chunk$set(cache=TRUE)
```

# Multiple Regression

## Multiple Regression

- We have been discussing simple models so far. 
- This works well when you have:
    - Randomized Data to test between specific groups (Treatment vs Control)
- In most situations we need look at more than just one relationship. 
- Think of this as needing more information to tell the entire story. 



## Motivating Example

- Health disparities are very real and exist across individuals and populations. 
- Before developing methods of remedying these disparities we need to be able to identify where there are disparities.In this homework we will consider a study by [(Asch & Armstrong, 2007)](http://www.ncbi.nlm.nih.gov/pubmed/17513818).  
- This paper considers 222 patients with localized prostate cancer. 




## Motivating Example 

- The table below partitions patients by race, hospital and whether or not the patient received a prostatectomy. 

|       |   Race | Prostatectomy | No Prostatectomy | 
| -------------- | -------- | ---------- | ------------ |
University Hospital | White | 54 | 37 | 
|  | Black | 7 | 5  |
| VA Hospital | White | 11 | 29 | 
| | Black | 22 | 57 | 








## Loading the Data


You can load this data into R with the code below:


```{r}
phil_disp <- read.table("https://drive.google.com/uc?export=download&id=0B8CsRLdwqzbzOXlIRl9VcjNJRFU", header=TRUE, sep=",")
```




## The Data 

This dataset contains the following variables: 


| Variable |       Description | 
| ----------- | -------------------- | 
| hospital  |     0 - University Hospital |
|           |     1 - VA Hospital | 
| race   |      0 - White |
|      |        1 - Black | 
| surgery |       0 - No prostatectomy |
|          |    1 - Had Prostatectomy | 
| number    |    Count of people in Category |





## Consider Prostatectomy by Race

```{r}
library(broom)
prost_race <- glm(surgery ~ race, weight=number, data= phil_disp,
                  family="binomial")
tidy(prost_race, exponentiate=T, conf.int=T)[,-c(3:4)]
```





## Consider Prostatectomy by Race

- What can we conclude? 
- What kind of policy might we want to invoke based on this discovery?




## Consider Prostatectomy by Hospital

```{r}
prost_hosp <- glm(surgery ~ hospital, weight=number, data= phil_disp,
                  family="binomial")
tidy(prost_hosp, exponentiate =T, conf.int=T)[,-c(3:4)]
```





## Consider Prostatectomy by Hospital

- What can we conclude? 




## Multiple Regression of Prostatectomy


```{r}
prost <- glm(surgery ~ hospital + race, weight=number, data= phil_disp,
             family="binomial")
tidy(prost, exponentiate=T, conf.int=T)[,-c(3:4)]
```





## Multiple Regression of Prostatectomy

- What can We conclude?
- What happened here?
- Does this change our policy suggestion from before?





## Benefits of Multiple Regression


- Multiple Regression helps us tell a more complete story. 
- Multiple regression controls for confounding. 





## Confounding

- Associated with both the Exposure and the Outcome
- Even if the Exposure and Outcome are not related, unmeasured confounding can show that they are. 





## What Do We Do with Confounding?

- We must add all confounders into our model. 
- Without adjusting for confounders are results may be highly biased. 
- Without adjusting for confounding we may make incorrect policies that do not fix the problem. 




## Multiple Linear Regression with appearances



- First start with univariate models
- Then perform the multiple model







## Multivariate Models


```{r}
library(broom)
library(fivethirtyeight)
mod3 <- lm(appearances~publisher + year, data=comic_characters)
tidy3 <- tidy(mod3, conf.int=T)[,-c(3:4)]
tidy3
```


## Interpreting Multiple Coefficients

- The intercept is when all coefficients are zero. 
- Each other coefficient is interpreted in context to another. 


## Interpreting Multiple Coefficients: Our Example

- Intercept: DC average appearances at year 0. ***Not Meaningful***
- Publisher Coefficient: If we consider 2 characters in the same year, the character from Marvel will have on average 9.54 less appearances than the character from DC. 
- Year Coefficient: If we consider 2 characters from the same publisher, an increase in 1 year will lead to on average 0.62 less appearances. 



## Further Example: Bike Sharing Data


- We have hourly data spanning 2 years
- This dataset has the first 19 days of each month. 
- Goal is to find the total count of bike rented. 

## Further Example: Bike Sharing Data

| Data |  Fields |
| :------: | :----------------------|
| datetime | hourly date + timestamp  |
| season |   1 = spring, 2 = summer, 3 = fall, 4 = winter  |
| holiday |  whether the day is considered a holiday |
| workingday |  whether the day is neither a weekend nor holiday |



## Further Example: Bike Sharing Data

| Data |  Fields |
| :------: | :----------------------|
| weather |  1: Clear, Few clouds, Partly cloudy, Partly cloudy  |
| | 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist | 
| | 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds |
| | 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog |
| temp |  temperature in Celsius |



## Further Example: Bike Sharing Data

| Data |  Fields |
| :------: | :----------------------|
| atemp |  "feels like" temperature in Celsius |
| humidity |  relative humidity |
| windspeed |  wind speed|
| casual |  number of non-registered user rentals initiated |
| registered |  number of registered user rentals initiated |
| count |  number of total rentals |


## Univariate Regressions

```{r}
library(readr)
library(tidyverse)
bikes <- read_csv("bike_sharing.csv") %>%
        mutate(season = as.factor(season)) %>%
        mutate(weather=as.factor(weather)) 
       
```


## Univariate Regressions

```{r}
mod1 <- lm(count~season, data=bikes)
mod2 <- lm(count~holiday, data=bikes)
mod3 <- lm(count~workingday, data=bikes)
mod4 <- lm(count~weather, data=bikes)
mod5 <- lm(count~temp, data=bikes)
mod6 <- lm(count~atemp, data=bikes)
mod7 <- lm(count~humidity, data=bikes)
mod8 <- lm(count~windspeed, data=bikes)
mod9 <- lm(count~casual, data=bikes)
mod10 <- lm(count~registered, data=bikes)
```


## Univariate Regressions

```{r, eval=F}
library(broom)
tidy1 <- tidy( mod1, conf.int=T )[-1, -c(3:4) ]
tidy2 <- tidy(mod2, conf.int=T )[-1, -c(3:4) ]
tidy3 <- tidy(mod3 , conf.int=T)[-1, -c(3:4) ]
tidy4 <- tidy(mod4 , conf.int=T)[-1, -c(3:4) ]
tidy5 <- tidy(mod5, conf.int=T)[-1, -c(3:4) ]
tidy6 <- tidy(mod6 , conf.int=T)[-1, -c(3:4) ]
tidy7 <- tidy(mod7 , conf.int=T)[-1, -c(3:4) ]
tidy8 <- tidy(mod8 , conf.int=T)[-1, -c(3:4) ]
tidy9 <- tidy(mod9, conf.int=T)[-1, -c(3:4) ]
tidy10 <- tidy(mod10, conf.int=T)[-1, -c(3:4) ]
bind_rows(tidy1, tidy2, tidy3, tidy4, tidy5, tidy6, tidy7, tidy8, tidy9, tidy10) 

```


## Univariate Regressions

```{r, echo=F}
library(broom)
tidy1 <- tidy( mod1, conf.int=T )[-1, -c(3:4) ]
tidy2 <- tidy(mod2, conf.int=T )[-1, -c(3:4) ]
tidy3 <- tidy(mod3 , conf.int=T)[-1, -c(3:4) ]
tidy4 <- tidy(mod4 , conf.int=T)[-1, -c(3:4) ]
tidy5 <- tidy(mod5, conf.int=T)[-1, -c(3:4) ]
tidy6 <- tidy(mod6 , conf.int=T)[-1, -c(3:4) ]
tidy7 <- tidy(mod7 , conf.int=T)[-1, -c(3:4) ]
tidy8 <- tidy(mod8 , conf.int=T)[-1, -c(3:4) ]
tidy9 <- tidy(mod9, conf.int=T)[-1, -c(3:4) ]
tidy10 <- tidy(mod10, conf.int=T)[-1, -c(3:4) ]
rbind(tidy1, tidy2, tidy3, tidy4, tidy5, tidy6, tidy7, tidy8, tidy9, tidy10)
```


## Multivariate

```{r, eval=F}
mod.final <- lm(count~season+weather+humidity+windspeed, data=bikes)
tidy(mod.final)[-1,-c(3:4)]
glance(mod.final)
```

## Multivariate

```{r, echo=F}
mod.final <- lm(count~season+weather+humidity+windspeed, data=bikes)
tidy(mod.final)[-1,-c(3:4)]
```


## Multivariate

```{r, echo=F}
glance(mod.final)
```
          
# Inference on Linear Regressions

## Inference on Linear Regressions

1. Overall F Test of Model
2. Individual Coefficient Tests
3. Testing Groups of Variables



## Overall Model F test

- We can perform an overall F Test for a model.
- When we do this we test the following Hypothesis
$$H_0:\beta_1=\beta_2=\cdots=\beta_p=0$$
$$H_1=\text{ at least one }\beta_i\ne0$$

## Overall Model F test: Bike Sharing

```{r, eval=F}
glance(mod.final)

```

## Overall Model F test: Bike Sharing

```{r, echo=F}
glance(mod.final)

```

## Overall Model F test: Bike Sharing

- We have an F Statistic of 3329.3
- This yields a p-value of $<0.0001$
- We can reject the null in favor of the alternative hypothesis. 
- This suggests that at least one $\beta_I$ is not 0. 

## Individual Coefficients $t$-test

- We can test each individual coefficients. 
- The hypothesis we test is that:
$$H_0:\beta_i=0$$
$$H_1=\beta_i\ne0$$
- We do this with a t-test. 

## Individual Coefficients $t$-test

- With the t-test we have that:
$$ t_i=\dfrac{\beta_i}{se(\beta_i)}$$
- Then we can test this with the $t$-distribution. 


## Individual Coefficients $t$-test

- Consider out Bike model:

$$ 
\begin{align}
E[count] &= \beta_0 +  \beta_1 season(Summer) + \beta_2 season(Fall) + \\
& \beta_3 season(Winter)  +    \beta_4 weather(2) +  \beta_5 weather(3) + \\
&  \beta_6 weather(4) + \beta_7 humidity + \beta_8 windspeed\\
\end{align}
$$

```{r, eval=F}
tidy(mod.final)
```



## Individual Coefficients $t$-Test

```{r, echo=F}
tidy(mod.final)
```

## F-test for Groups of Coefficients

- Many times we want to be able to test the significance of groups of coefficients. 
- We can do this with an F-test as well. 
- For example we may want to test that:
$$H_0:\beta_1=\beta_2=0$$
$$H_1:\text{ at least 1 }\beta_i\ne0$$

## Groups of Coefficients Example

- Consider `Season` in our bike example. 
- Only the first coefficient is significant. 
- We may want to know if we the whole variable is worth having in the model. 
- We will use the `anova()` function in R.


## Groups of Coefficients Example

```{r,eval=F}
mod1 <- lm(count~season+weather+humidity+windspeed, data=bikes)
mod2 <- lm(count~weather+humidity+windspeed, data=bikes)
anova(mod1, mod2)
```


## Groups of Coefficients Example

```{r,echo=F}
mod1 <- lm(count~season+weather+humidity+windspeed, data=bikes)
mod2 <- lm(count~weather+humidity+windspeed, data=bikes)
anova(mod1, mod2)
```
## Groups of Coefficients Example 2

- Consider `weather` in our bike example. 
- Only the first coefficient is significant. 
- We may want to know if we the whole variable is worth having in the model. 
- We will use the `anova()` function in R.


## Groups of Coefficients Example 2

```{r,eval=F}
mod1 <- lm(count~season+weather+humidity+windspeed, data=bikes)
mod2 <- lm(count~season+humidity+windspeed, data=bikes)
anova(mod1, mod2)
```


## Groups of Coefficients Example 2

```{r,echo=F}
mod1 <- lm(count~season+weather+humidity+windspeed, data=bikes)
mod2 <- lm(count~season+humidity+windspeed, data=bikes)
anova(mod1, mod2)
```