---
title: "Multicollinearity"
author: "Adam J Sullivan, PhD"
date: "02/14/2018"
output:
   ioslides_presentation: 
    widescreen: true
notes: ''
link: yes
slides: yes
layout: page
css:  "C:/Users/adam_/Dropbox (Personal)/Brown/Teaching/Brown Courses/PHP2511/Spring 2018/website/php-1511-2511.github.io/Notes/slides.css"
---

```{r setup, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(error = TRUE)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_chunk$set(results="hold")
knitr::opts_chunk$set(cache=F)
```


# Multicollinearity 



## What is Multicollinearity

- Multicollinearity exists when 2 or more covariates in a model are moderately or highly correlated. 
- This may be viewed as an easy issue to deal with as many things we may want to control for are just highly correlated. 
- For example, education and income are highly correlated. 


## Types of Multicollinearity

- Data based: 
    - Could be poorly designed study
    - observational data where only variables collected are all correlated. 
- Structural:
    - Duplicate variables so they are mathematically the same. 
    - Variables that were created from others
        - For example, weight and height are highly correlated with BMI. 


## Consider the following data:

- This data has been simulated so that it is not collinear:

```{r, echo=F}
set.seed(456)
Runc = matrix(cbind(1,.80,.2,  .80,1,0,  .2,0,1),nrow=3)
U = t(chol(Runc))
Unvars = dim(U)[1]
numobs = 100000
set.seed(1)
random.normal = matrix(rnorm(Unvars*numobs,0,1), nrow=Unvars, ncol=numobs);
X = U %*% random.normal
newX = t(X)
raw = as.data.frame(newX)
orig.raw = as.data.frame(t(random.normal))
names(raw) = c("response","predictor1","predictor2")
#Correlation 
round(cor(raw),3)
```
- Let's look at the regressions

## Regression on Uncorrelated Data

- We will consider the following regressions:
$$\text{Model 1: }Response = \hat{\beta}_0 + \hat{\beta}_1Predictor_1$$
$$\text{Model 2: }Response = \hat{\beta}_0 + \hat{\beta}_1Predictor_2$$
$$\text{Model 3: }Response = \hat{\beta}_0 + \hat{\beta}_1Predictor_1+ + \hat{\beta}_2Predictor_2$$
$$\text{Model 4: }Response = \hat{\beta}_0 + \hat{\beta}_1Predictor_2 + \hat{\beta}_2Predictor_1$$



## Regression on Uncorrelated Data

```{r, echo=F}
library(broom)
library(dplyr)
mod1 <- lm(data=raw, response~predictor1 )
mod2 <- lm(data=raw, response~predictor2)
mod3 <- lm(data=raw, response~predictor1 + predictor2)
mod4 <- lm(data=raw, response~predictor2 + predictor1)

tidy1 <- tidy(mod1, conf.int=T)[-1,-c(3:4)]
tidy2 <- tidy(mod2, conf.int=T)[-1,-c(3:4)]
tidy3 <- tidy(mod3, conf.int=T)[-1,-c(3:4)]
tidy4 <- tidy(mod4, conf.int=T)[-1,-c(3:4)]

knitr::kable(bind_rows(tidy1,tidy2,tidy3, tidy4))
```


## Regression on Uncorrelated Data

```{r, echo=F}
glance1 <- glance(mod1)[,c(1:5)]
glance2 <- glance(mod2)[,c(1:5)]
glance3 <- glance(mod3)[,c(1:5)]
glance4 <- glance(mod4)[,c(1:5)]

knitr::kable(bind_rows(glance1,glance2,glance3, glance4))
```

## Sum Squares of Models

```{r, echo=F}
tidy11 <- tidy(anova(mod1))[1, c(1:4)]
tidy22 <- tidy(anova(mod2))[1, c(1:4)]
tidy33 <- tidy(anova(mod3))[c(1:2), c(1:4)]
tidy44 <- tidy(anova(mod4))[c(1:2), c(1:4)]

knitr::kable(bind_rows(tidy11,tidy22,tidy33, tidy44))
```


## What Do We Notice?

- Coefficients do not change in models. 
- Sums of Squares added to model remain consistent


## Consider the following data:

- This data has been simulated so that it is highly collinear:

```{r, echo=F}
Rcor = matrix(cbind(1,.80,.2,  .80,1,.7,  .2,.7,1),nrow=3)
U = t(chol(Rcor))
Unvars = dim(U)[1]
numobs = 100
set.seed(1)
random.normal = matrix(rnorm(Unvars*numobs,0,1), nrow=Unvars, ncol=numobs);
X = U %*% random.normal
newX = t(X)
raw.corr = as.data.frame(newX)
orig.raw = as.data.frame(t(random.normal))
names(raw.corr) = c("response","predictor1","predictor2")
#Correlation 
round(cor(raw.corr),3)
```
- Let's look at the regressions

## Regression on Correlated Data

- We will consider the following regressions:
$$\text{Model 1: }Response = \hat{\beta}_0 + \hat{\beta}_1Predictor_1$$
$$\text{Model 2: }Response = \hat{\beta}_0 + \hat{\beta}_1Predictor_2$$
$$\text{Model 3: }Response = \hat{\beta}_0 + \hat{\beta}_1Predictor_1+ + \hat{\beta}_2Predictor_2$$
$$\text{Model 4: }Response = \hat{\beta}_0 + \hat{\beta}_1Predictor_2+ + \hat{\beta}_2Predictor_1$$



## Regression on Correlated Data


```{r, echo=F}
library(broom)
library(dplyr)
mod1c <- lm(data=raw.corr, response~predictor1 )
mod2c <- lm(data=raw.corr, response~predictor2)
mod3c <- lm(data=raw.corr, response~predictor1 + predictor2)
mod4c <- lm(data=raw.corr, response~predictor2 + predictor1)

tidy1c <- tidy(mod1c, conf.int=T)[-1,-c(3:4)]
tidy2c <- tidy(mod2c, conf.int=T)[-1,-c(3:4)]
tidy3c <- tidy(mod3c, conf.int=T)[-1,-c(3:4)]
tidy4c <- tidy(mod4c, conf.int=T)[-1,-c(3:4)]

knitr::kable(bind_rows(tidy1c,tidy2c,tidy3c, tidy4c))
```


## Regression on Correlated Data

```{r, echo=F}
glance1c <- glance(mod1c)[,c(1:5)]
glance2c <- glance(mod2c)[,c(1:5)]
glance3c <- glance(mod3c)[,c(1:5)]
glance4c <- glance(mod4c)[,c(1:5)]

knitr::kable(bind_rows(glance1c,glance2c,glance3c, glance4c))
```


## Sum Squares of Models

```{r, echo=F}
tidy11c <- tidy(anova(mod1c))[1, c(1:4)]
tidy22c <- tidy(anova(mod2c))[1, c(1:4)]
tidy33c <- tidy(anova(mod3c))[c(1:2), c(1:4)]
tidy44c <- tidy(anova(mod4c))[c(1:2), c(1:4)]

knitr::kable(bind_rows(tidy11c,tidy22c,tidy33c, tidy44c))
```

## What did we Notice?

- Coefficients change a lot
- Sum of Squares Depends on the order in which data is in the model. 



## Signs of Multicollinearity

- Estimates of the coefficients vary from model to model.
- $t$-tests of individual slopes are non-significant but overall F-test is significant. 
- Correlations among covariates are large. 


## How Can we Detect this?

- Consider the model with just one covariate:

$$y_i= \beta_0 + \beta_kx_{ik} + \varepsilon_i$$
- We can see this variance:
$$Var(b_k)_{min}= \dfrac{\sigma^2}{\sum_{i=1}^n(x_{ik}-\bar{x}_k)^2}$$
- This is the smallest variance will be. 

## Then the larger model


- Consider the model with just one covariate:

$$y_i= \beta_0 + \beta_1x_{i1} + \cdots +  \beta_kx_{ik} + \cdots \beta_px_{ip} \varepsilon_i$$
- We can see this variance:
$$Var(b_k)= \dfrac{\sigma^2}{\sum_{i=1}^n(x_{ik}-\bar{x}_k)^2}\times\dfrac{1}{1-R^2_k}$$
- $R^2_k$ is the $R^2$ value of the k$^{th}$ predictor on the remaining. 


## What does this tell us?

- How much is our variance inflated by?

$$\dfrac{Var(b_k)}{Var(b_k)_{min}} =\dfrac{1}{1-R^2_k} $$

- Variance Inflation Factor
$$VIF_k=\dfrac{1}{1-R^2_k}$$


## Variance Inflation Factor

- Rule of thumb
    - 1 = not correlated.
    - Between 1 and 5 = moderately correlated.
    - Greater than 5 = highly correlated.
- Some suggest anything more than 2.5 should cause concern and definitely over 10.

## Variance Inflation Factor

- Be careful just judging by it alone
- For example $x$ and $x^2$ may have a high VIF but this would not hurt your model. 
- Also Indicator variables often have a high VIF with each other but this is not an issue. 

## Calculating in R

```{r, eval=F}
library(car)
vif1 <- vif(mod3)
vif2 <- vif(mod4)
knitr::kable(bind_rows(vif1,vif2))
```




## Calculating in R

```{r, echo=F}
library(car)
vif1 <- vif(mod3)
vif2 <- vif(mod4)
knitr::kable(bind_rows(vif1,vif2))
```


## Calculating in R

```{r, echo=F}
library(car)
vif1 <- vif(mod3c)
vif2 <- vif(mod4c)
knitr::kable(bind_rows(vif1,vif2))
```

## How Can We Deal with it?

- Remove Multicollinear variables from model.
    - What might the effects of this be?
- Create a summed score of the collinear variables. 
- Create a score based on something like Principal Component Analysis. 


