---
title: "Outliers and Influential Observations"
author: "Adam J Sullivan, PhD"
date: "02/12/2018"
output:
   ioslides_presentation: 
    widescreen: true
notes: ''
link: yes
slides: yes
layout: page
css:  "C:/Users/adam_/Dropbox (Personal)/Brown/Teaching/Brown Courses/PHP2511/Spring 2018/website/php-1511-2511.github.io/Notes/slides.css"
---

```{r setup, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(error = TRUE)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_chunk$set(results="hold")
knitr::opts_chunk$set(cache=F)
```


```{r, echo=F}
library(tidyverse)
library(ggplot2)
library(broom)
mod1 <- lm(y1 ~ x1, data=anscombe)
mod2 <- lm(y2 ~ x2, data=anscombe)
mod3 <- lm(y3 ~ x3, data=anscombe)
mod4 <- lm(y4 ~ x4, data=anscombe)
```

```{r,echo=F}
library(broom)
library(dplyr)
tidy1 <- tidy(mod1, conf.int = T)
tidy2 <- tidy(mod2, conf.int = T)
tidy3 <- tidy(mod3, conf.int = T)
tidy4 <- tidy(mod4, conf.int = T)
glance1 <- glance(mod1)
glance2 <- glance(mod2)
glance3 <- glance(mod3)
glance4 <- glance(mod4)
```

## Leverage and Outliers

- We will now move onto leverage points and outliers. 
- **Leverage point**: is a value of the predictor that is far from the average of the predictor variables. 
- **Outlier points**: is a values of the outcome that is far from the average of the outcome. 




## Leverage and Outliers

- These two things together help us determine whether certain points have a lot of influence on our regression model. 
- For example in Anscombe model 3 it appears that there is one point that not only is an outlier but may be a leverage point as well. 
- Instead of trying to parse both of these concepts out we will focus on a plot that helps us consider influential points as a whole. 



## Cook's D Plot For Models 1 and 3

```{r, eval=F}
par(mfrow=c(1,2))
plot(mod1, 5)
plot(mod3, 5)
```
  



## Cook's D Plot

```{r, eval=F}
par(mfrow=c(1,2))
plot(mod1, 5)
plot(mod3, 5)
```


## Cook's D 

- Cook's distance attempts to tell us how much $\hat{\beta}$ changes due to the inclusion of the $i^{th}$ observation. 
- We are concerned with values that are near or greater than 1 when dealing with standardized residuals.
- The previous figures shows that for model 1 the largest  is 0.5 however for model 3 the 3rd observation is an influential point being that it is close to 1.5.

## What can we do with this point?

- We can decide to remove the point and re-run the regression. 

```{r, eval=F}
mod3a <- lm(y3~x3, data=anscombe[-3,])

tidy3a <- tidy(mod3a, conf.int = T)
knitr::kable(bind_rows(tidy3, tidy3a)[-c(3,4)])

glance3a <- glance(mod3a)
knitr::kable(bind_rows(glance3, glance3a))
```


## What can we do with this point?

```{r, echo=F}
mod3a <- lm(y3~x3, data=anscombe[-3,])
tidy3a <- tidy(mod3a, conf.int = T)
knitr::kable(bind_rows(tidy3, tidy3a)[-c(3,4)])
```


## What can we do with this point?

```{r, echo=F}
glance3a <- glance(mod3a)
knitr::kable(bind_rows(glance3, glance3a))
```


## Marginal Model Plots

- We have been using this example of Systolic Blood Pressure regressed onto BMI and Age.
- It may not have been clear as to how we knew that we needed to consider transforming the data and that our model may not be sufficient. 
- We will consider the next level of plots called Marginal Model Plots. 
- The aim of these plots is to show how well out model fits the data. 


## Marginal Model Plots

```{r, echo=F}
library(haven)
fhs <- read_dta("fhs.dta")
mod <- lm(sysbp1 ~ bmi1 + age1, data=fhs)
fhs <- fhs %>% mutate(sysbp1t = log(sysbp1))
mod_syst <- lm(sysbp1t ~ bmi1 + age1, data=fhs)
```

```{r, eval=F}
  library(car)
mmps(mod)
```




## Marginal Model Plots

```{r, echo=F}
  library(car)
mmps(mod)
```



## What Can we See?

- From the figure we can see that the blue line represents a loess(smoothing) line for the data and the dashed line represents the model which R fitted. 
- If we consider BMI we can see that for large BMI values our model gives values lower than actual data values. 
- Also we can see that the loess line is more curved than our data model. 
- For age the fit appears to be good but overall for the predicted values we are seeing the curve that our model does not represent. 


## What happens when we Transform?

- We then transform the data and run these plots again.  
```{r, eval=F}
mmps(mod_syst)
```


## What happens when we Transform?


```{r, echo=F}
mmps(mod_syst)
```



## What Happens when We Transform?

- The results of the transformation. We already saw that this transformation made systolic blood pressure normally distributed and now we can see the impact of this on the model. 
- All 3 plots show a better fit than they did with the untransformed data. 



## Using Marginal Model Plots with Outliers

- We can also use Marginal Model Plots with Outliers:

```{r, eval=F}
mmps(mod3)
mmps(mod3a)
```




## Using Marginal Model Plots with Outliers

```{r, echo=F}
mmps(mod3)
```



## Using Marginal Model Plots with Outliers

```{r, echo=F}
mmps(mod3a)
```


