<!DOCTYPE html>
<html>
<head>
  <title>Resampling</title>
  <meta charset="utf-8">
  <meta name="description" content="Resampling">
  <meta name="author" content="Adam J Sullivan">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/github.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->  <link rel=stylesheet href="libraries/widgets/quiz/css/demo.css"></link>
<link rel=stylesheet href="libraries/widgets/bootstrap/css/bootstrap.css"></link>
<link rel=stylesheet href="libraries/widgets/interactive/css/aceeditor.css"></link>
<link rel=stylesheet href="./assets/css/ribbons.css"></link>
<link rel=stylesheet href="./assets/css/style.css"></link>

  
  <!-- Grab CDN jQuery, fall back to local if offline -->
  <script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
  <script>window.jQuery || document.write('<script src="libraries/widgets/quiz/js/jquery.js"><\/script>')</script> 
  <script data-main="libraries/frameworks/io2012/js/slides" 
    src="libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
  

</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
      <slide class="nobackground">
    <article class="flexbox vcenter">
      <span>
        <img width='300px' src="assets/img/publichealthlogo.png">
      </span>
    </article>
  </slide>
    <slide class="title-slide segue nobackground">
  <aside class="gdbar">
    <img src="assets/img/publichealthlogo.png">
  </aside>
  <hgroup class="auto-fadein">
    <h1>Resampling</h1>
    <h2></h2>
    <p>Adam J Sullivan<br/>Assistant Professor of Biostatistics<br/>Brown University</p>
  </hgroup>
  <article></article>  
</slide>
    

    <!-- SLIDES -->
    <slide class="segue" id="slide-1" style="background:grey;">
  <hgroup>
    <h1>Resampling Methods</h1>
    <hr>
  </hgroup>
  <article data-timings="">
    
    
    <footer class = 'logo'>
<div style="position: absolute; left: 900px; top: 600px; z-index:100">
<img src = "assets/img/publichealthlogo.png" height="100" >
</div>
</footer>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Resampling Methods</h2>
    <hr>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Very popular tool in modern statistics. </li>
<li>Involvees drawing samples from data and calculating statistics in the sample. </li>
<li>Quick Example: Getting standard error and confidence intervals for non-normal data in linear regression</li>
</ul>

    
    <footer class = 'logo'>
<div style="position: absolute; left: 900px; top: 600px; z-index:100">
<img src = "assets/img/publichealthlogo.png" height="100" >
</div>
</footer>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Resampling Methods</h2>
    <hr>
  </hgroup>
  <article data-timings="">
    <p><img src="bootstrap.png" alt=""></p>

    
    <footer class = 'logo'>
<div style="position: absolute; left: 900px; top: 600px; z-index:100">
<img src = "assets/img/publichealthlogo.png" height="100" >
</div>
</footer>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Why Resampling {#why}</h2>
    <hr>
  </hgroup>
  <article data-timings="">
    <p>Thus far, in our tutorials we have been using the <em>validation</em> or <em>hold-out</em> approach to estimate the prediction error of our predictive models. This involves randomly dividing the available set of observations into two parts, a <em>training set</em> and a <em>testing set</em> (aka <em>validation set</em>). Our statistical model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set. The resulting validation set error rate (typically assessed using MSE in the case of a quantitative response) provides an estimate of the test error rate.  </p>

<p>The validation set approach is conceptually simple and is easy to implement. But it has two potential drawbacks:</p>

<p>First, the estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.  I will illustrate on our <code>auto</code> data set.  Here we see that there is a relationship between <em>mpg</em> and <em>horsepower</em> and it doesn&#39;t seem linear but we&#39;re not sure which polynomial degree creates the best fit.</p>

<pre><code class="r">ggplot(auto, aes(horsepower, mpg)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, se = FALSE) +
  geom_smooth(method = &quot;lm&quot;, formula = y ~ poly(x, 2), se = FALSE, linetype = 2) +
  geom_smooth(method = &quot;lm&quot;, formula = y ~ poly(x, 3), se = FALSE, linetype = 3) +
  geom_smooth(method = &quot;lm&quot;, formula = y ~ poly(x, 4), se = FALSE, linetype = 4)
</code></pre>

<p><img src="/public/images/analytics/resampling/unnamed-chunk-2-1.png" style="display: block; margin: auto;" /></p>

<p>Let&#39;s go ahead and do the traditional validation set approach to split our data into a training and testing set.  Then we&#39;ll fit 10 different models ranging from a linear model to a 10th degree polynomial model. The results show us there is a steep decline in our test error (MSE) rate when we go from a linear model to a quadratic model; however, the MSE flatlines beyond that point suggesting that adding more polynomial degrees likely does not improve the model performance. </p>

<pre><code class="r">set.seed(1)
sample &lt;- sample(c(TRUE, FALSE), nrow(auto), replace = T, prob = c(0.6,0.4))
train &lt;- auto[sample, ]
test &lt;- auto[!sample, ]

# loop for first ten polynomial
mse.df &lt;- tibble(degree = 1:10, mse = NA)

for(i in 1:10) {
  lm.fit &lt;- lm(mpg ~ poly(horsepower, i), data = train)
  mse.df[i, 2] &lt;- mean((test$mpg - predict(lm.fit, test))^2)
}

ggplot(mse.df, aes(degree, mse)) +
  geom_line() +
  geom_point() +
  ylim(c(10, 30))
</code></pre>

<p><img src="/public/images/analytics/resampling/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /></p>

<p>However, our MSE is dependent on our training and test samples. If we repeat the process of randomly splitting the sample set into two parts, we will get a somewhat different estimate for the test MSE each time. I illustrate below, which displays ten different validation set MSE curves from the auto data set, produced using ten different random splits of the observations into training and validation sets. All ten curves indicate that the model with a quadratic term has a dramatically smaller validation set MSE than the model with only a linear term. Furthermore, all ten curves indicate that there is not much benefit in including cubic or higher-order polynomial terms in the model. But it is worth noting that each of the ten curves results in a <strong><em>different test MSE estimate</em></strong> for each of the ten regression models considered. And there is no consensus among the curves as to which model results in the smallest validation set MSE.</p>

<pre><code class="r">mse.df.2 &lt;- tibble(sample = vector(&quot;integer&quot;, 100), 
                   degree = vector(&quot;integer&quot;, 100), 
                   mse = vector(&quot;double&quot;, 100))
counter &lt;- 1

for(i in 1:10) {
  # random sample
  set.seed(i)
  sample &lt;- sample(c(TRUE, FALSE), nrow(auto), replace = T, prob = c(0.6,0.4))
  train &lt;- auto[sample, ]
  test &lt;- auto[!sample, ]

  # modeling
  for(j in 1:10) {
    lm.fit &lt;- lm(mpg ~ poly(horsepower, j), data = train)

    # add degree &amp; mse values
    mse.df.2[counter, 2] &lt;- j
    mse.df.2[counter, 3] &lt;- mean((test$mpg - predict(lm.fit, test))^2)

    # add sample identifier
    mse.df.2[counter, 1] &lt;- i
    counter &lt;- counter + 1
  }
  next
}

ggplot(mse.df.2, aes(degree, mse, color = factor(sample))) +
  geom_line(show.legend = FALSE) +
  geom_point(show.legend = FALSE) +
  ylim(c(10, 30))
</code></pre>

<p><img src="/public/images/analytics/resampling/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /></p>

<p>A second concern with the validation approach, only a subset of the observations, those that are included in the training set rather than in the validation set, are used to fit the model. Since statistical methods tend to perform worse when trained on fewer observations, this suggests that the validation set error rate may tend to overestimate the test error rate for the model fit on the entire data set.</p>

<p>We can address these concerns using <em>cross-validation</em> methods.</p>

<h2>Leave-One-Out Cross-Validation {#LOOCV}</h2>

<p><em>Leave-one-out cross-validation</em> (LOOCV) is closely related to the validation set approach as it involves splitting the set of observations into two parts. However, instead of creating two subsets of comparable size (i.e. 60% training, 40% validation), a single observation (\[x_1, y_1\]) is used for the validation set, and the remaining \[n-1\] observations {\[(x_2, y_2), \dots, (x_n, y_n)\]} make up the training set. The statistical learning method is fit on the \[n − 1\] training observations, and a prediction \[\hat y_1\] is made for the excluded observation. Since the validation observation (\[x_1, y_1\]) was not used in the fitting process, the estimate error \[MSE_1 = (y_1 − \hat y_1)^2\] provides an approximately unbiased estimate for the test error. But even though \[MSE_1\] is unbiased for the test error, it is a poor estimate because it is highly variable, since it is based upon a single observation (\[x_1, y_1\]).</p>

<p>However, we can repeat the procedure by selecting a different row (\[x_2, y_2\]) for the validation data, training the statistical learning procedure on the other \[n-1\] observations and computing \[MSE_2 =(y_2− \hat y_2)^2\]. We can repeate this approach <em>n</em> times, where each time we holdout a different, single observation to validate on.  This produces a total of <em>n</em> squared errors, \[MSE_1,\dots, MSE_n\]. The LOOCV estimate for the test MSE is the average of these <em>n</em> test error estimates:</p>

<p>\[ CV_{(n)} = \frac{1}{n}\sum^n_{i=1}MSE_i  \tag{1}\]</p>

<p>To perform this procedure in R we first need to understand an important nuance.  In the logistic regression tutorial, we used the <code>glm</code> function to perform logistic regression by passing in the <code>family = &quot;binomial&quot;</code> argument. But if we use <code>glm</code> to fit a model without passing in the family argument, then it performs linear regression, just like the <code>lm</code> function. So, for instance:</p>

<pre><code class="r">glm.fit &lt;- glm(mpg ~ horsepower, data = auto)
coef(glm.fit)
## (Intercept)  horsepower 
##  39.9358610  -0.1578447
</code></pre>

<p>is the same as</p>

<pre><code class="r">lm.fit &lt;- lm(mpg ~ horsepower, data = auto)
coef(lm.fit)
## (Intercept)  horsepower 
##  39.9358610  -0.1578447
</code></pre>

<p>Why is this important?  Because we can perform LOOCV for any generalized linear model using <code>glm</code> and the <code>cv.glm</code> function from the <a href="http://cran.r-project.org/web/packages/boot/index.html"><code>boot</code></a> package. <code>boot</code> provides extensive facilities for bootstrapping and related resampling methods. You can bootstrap a single statistic (e.g. a median), a vector (e.g., regression weights), or as you&#39;ll see in this tutorial perform cross-validation. To perform LOOCV for a given generalized linear model we simply:</p>

<ol>
<li>fit our model across the entire data set with <code>glm</code></li>
<li>feed the entire data set and our fitted model into <code>cv.glm</code></li>
</ol>

<pre><code class="r"># step 1: fit model
glm.fit &lt;- glm(mpg ~ horsepower, data = auto)

# setp 2: perform LOOCV across entire data set
loocv.err &lt;- cv.glm(auto, glm.fit)

str(loocv.err)
## List of 4
##  $ call : language cv.glm(data = auto, glmfit = glm.fit)
##  $ K    : num 392
##  $ delta: num [1:2] 24.2 24.2
##  $ seed : int [1:626] 403 392 -1703707781 1994959178 434562476 -1277611857 -1105401243 1020654108 526650482 -1538305299 ...
</code></pre>

<p><code>cv.glm</code> provides a list with 4 outputs:</p>

<ol>
<li><em>call</em>: the original function call</li>
<li><em>K</em>: the number of <em>folds</em> used.  In our case it is 392 because the LOOCV looped through and pulled out each observation at least once to use a test observation.</li>
<li><em>delta</em>: the cross-validation estimate of prediction error.  The first number, which is the primary number we care about, is the output from Eq. 1 listed above.</li>
<li><em>seed</em>: the values of the random seed used for the function call</li>
</ol>

<p>The result we primarily care about is the cross-validation estimate of test error (Eq. 1).  Our cross-validation estimate for the test error is approximately 24.23.  This estimate is a far less biased estimate of the test error compared to our single test MSE produced by a training - testing validation approach. </p>

<pre><code class="r">loocv.err$delta[1]
## [1] 24.23151
</code></pre>

<p>We can repeat this procedure to estimate an ubiased MSE across multiple model fits. For example, to assess multiple polynomial fits (as we did above) to identify the one that represents the best fit we can integrate this procedure into a function.  Here we develop a function that computes the LOOCV MSE based on specified polynomial degree.  We then feed this function (via <code>map_dbl</code>) values 1-5 to compute the first through fifth polynomials.  </p>

<pre><code class="r"># create function that computes LOOCV MSE based on specified polynomial degree
loocv_error &lt;- function(x) {
  glm.fit &lt;- glm(mpg ~ poly(horsepower, x), data = auto)
  cv.glm(auto, glm.fit)$delta[1]
}

# compute LOOCV MSE for polynomial degrees 1-5
library(purrr)
1:5 %&gt;% map_dbl(loocv_error)
## [1] 24.23151 19.24821 19.33498 19.42443 19.03321
</code></pre>

<p>Our results illustrate a sharp drop in the estimated test MSE between the linear and quadratic fits, but then no clear improvement from using higher-order polynomials.  Thus, our unbiased MSEs suggest that using a 2nd polynomial (quadratic fit) is likely the optimal model balancing interpretation and low test errors.  </p>

<p>This LOOCV approach can be used with any kind of predictive modeling. For example we could use it with logistic regression or linear discriminant analysis.  Unfortunately, this can be very time consuming approach if <em>n</em> is large, you&#39;re trying to loop through many models (i.e. 1-10 polynomials), and if each individual model is slow to fit. For example, if we wanted to perform this approach on the <code>ggplot2::diamonds</code> data set for a linear regression model, which contains 53,940 observations, the computation time is nearly 30 minutes!</p>

<pre><code class="r"># DO NOT RUN THIS CODE - YOU WILL BE WAITING A LONG TIME!!
system.time({
    diamonds.fit &lt;- glm(price ~ carat + cut + color + clarity, data = diamonds)
    cv.glm(diamonds, diamonds.fit)
})
#      user    system    elapsed 
#  1739.041   285.496   2035.062 
</code></pre>

<h2><em>k</em>-Fold Cross Validation {#kfold}</h2>

<p>An alternative to LOOCV is the <em>k</em>-fold cross validation approach. This resampling method involves randomly dividing the data into <em>k</em> groups (aka <em>folds</em>) of approximately equal size. The first fold is treated as a validation set, and the statistical method is fit on the remaining data. The mean squared error, \[MSE_1\], is then computed on the observations in the held-out fold. This procedure is repeated <em>k</em> times; each time, a different group of observations is treated as the validation set. This process results in <em>k</em> estimates of the test error, \[MSE_1, MSE_2, \dots , MSE_k\]. Thus, the <em>k</em>-fold CV estimate is computed by averaging these values,</p>

<p>\[ CV_{(k)} = \frac{1}{k}\sum^k_{i=1}MSE_i  \tag{2} \]</p>

<p>It is not hard to see that LOOCV is a special case of <em>k</em>-fold approach in which <em>k</em>
is set to equal <em>n</em>. However, using the <em>k</em>-fold approach, one typically uses <em>k</em> = 5 or <em>k</em> = 10.  This can substantially reduce the computational burden of LOOCV.  Furthermore, there has been sufficient empirical evidence that demonstrates using 5-10 folds yield surprisingly accurate test error rate estimates (see chapter 5 of ISLR for more details[<sup>islr]).</sup> </p>

<p>We can implement the <em>k</em>-fold approach just as we did with the LOOCV approach.  The only difference is incorporating the <code>K = 10</code> argument that we include in the <code>cv.glm</code> function.  Below illustrates our <em>k</em>-fold MSE values for the different polynomial models on our auto data.  When compared to the LOOCV outputs we see that the results are nearly identical.</p>

<pre><code class="r"># create function that computes k-fold MSE based on specified polynomial degree
kfcv_error &lt;- function(x) {
  glm.fit &lt;- glm(mpg ~ poly(horsepower, x), data = auto)
  cv.glm(auto, glm.fit, K = 10)$delta[1]
}

# compute k-fold MSE for polynomial degrees 1-5
1:5 %&gt;% map_dbl(kfcv_error)
## [1] 24.56850 19.22648 19.29535 19.46601 19.24090

# compare to LOOCV MSE values
1:5 %&gt;% map_dbl(loocv_error)
## [1] 24.23151 19.24821 19.33498 19.42443 19.03321
</code></pre>

<p>We can also illustrate the computational advantage of the <em>k</em>-fold approach.  As we saw, using LOOCV on the <code>diamonds</code> data set took nearly 30 minutes whereas using the <em>k</em>-fold approach only takes about 4 seconds.</p>

<pre><code class="r">system.time({
  diamonds.fit &lt;- glm(price ~ carat + cut + color + clarity, data = diamonds)
  cv.glm(diamonds, diamonds.fit, K = 10)
})
##    user  system elapsed 
##   3.760   0.564   4.347
</code></pre>

<p>We can apply this same approach to classification problems as well. For example, in the previous tutorial we compared the performance of a logistic regression, linear discriminant analysis (LDA), and quadratic discriminant analysis (QDA) on some stock market data using the traditional training vs. testing (60%/40%) data splitting approach.  We could&#39;ve performed the same assessment using cross validation. In the classification setting, the LOOCV error rate takes the form</p>

<p>\[ CV_{(n)} = \frac{1}{n}\sum^n_{i=1}Err_i  \tag{3} \]</p>

<p>where \[Err_i = I(y_i \ne \hat y_i)\].  The <em>k</em>-fold CV error rate and validation set error rates are defined analogously.</p>

<p>Consequently, for the logistic regression we use <code>cv.glm</code> to perform a <em>k</em>-fold cross validation.  The end result is an estimated CV error rate of .5. (<em>Note: since the response variable is binary we incorporate a new cost function to compute the estimated error rate in Eq. 3</em>)</p>

<pre><code class="r">stock &lt;- ISLR::Smarket

# fit logistic regression model
glm.fit &lt;- glm(Direction ~ Lag1 + Lag2, family = binomial, data = stock)

# The cost function here correlates to that in Eq. 3
cost &lt;- function(r, pi = 0) mean(abs(r - pi) &gt; 0.5)

# compute the k-fold estimated error with our cost function
cv.glm(stock, glm.fit, cost, K = 10)$delta[1]
## [1] 0.4984
</code></pre>

<p>To performm cross validation with our LDA and QDA models we use a slightly different approach.  Both the <code>lda</code> and <code>qda</code> functions have built-in cross validation arguments.  Thus, setting <code>CV = TRUE</code> within these functions will result in a LOOCV execution and the class and posterior probabilities are a product of this cross validation.</p>

<pre><code class="r">library(MASS)

# fit discriminant analysis models with CV = TRUE for LOOCV
lda.fit &lt;- lda(Direction ~ Lag1 + Lag2, CV = TRUE, data = stock)
qda.fit &lt;- qda(Direction ~ Lag1 + Lag2, CV = TRUE, data = stock)

# compute estimated test error based on cross validation
mean(lda.fit$class != stock$Direction)
## [1] 0.4816
mean(qda.fit$class != stock$Direction)
## [1] 0.4872
</code></pre>

<p>Thus, the results are similar to what we saw in the previous tutorial, none of these models do an exceptional (or even decent!) job.  However, we see that the LOOCV estimated error for the QDA model (.487) is fairly higher than what we saw in the train-test validation approach (.40).  This suggests that our previous QDA model with the train-test validation approach may have been a bit optimistically biased!</p>

<h2>Bootstrapping {#boot}</h2>

<p><em>Bootstrapping</em> is a widely applicable and extremely powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method. As a simple example, bootstraping can be used to estimate the standard errors of the coefficients from a linear regression fit. In the case of linear regression, this is not particularly useful, since we saw in the <a href="linear_regression">linear regression tutorial</a> that R provides such standard errors automatically. However, the power of the bootstrap lies in the fact that it can be easily applied to a wide range of statistical learning methods, including some for which a measure of variability is otherwise difficult to obtain and is not automatically output by statistical software.</p>

<p>In essence bootstrapping repeatedly draws independent samples from our data set to create bootstrap data sets.  This sample is performed with <em>replacement</em>, which means that the same observation can be sampled more than once. The figure below from the ISLR[<sup>islr]</sup> book depicts the bootsrap approach on a small data set (<em>n = 3</em>).</p>

<p><center>
<img src="/public/images/analytics/resampling/bootstrap.png"  style="width: 60%; height: 60%;" />
</center></p>

<p>Each bootstrap data set (\[Z^{*1}, Z^{*2}, \dots, Z^{*B}\]) contains <em>n</em> observations, sampled with replacement from the original data set.  Each bootstrap is used to compute the estimated statistic we are interested in (\[\hat\alpha^*\]). We can then use all the bootstrapped data sets to compute the standard error of \[\hat\alpha^{*1}, \hat\alpha^{*2}, \dots, \hat\alpha^{*B}\] desired statistic as</p>

<p>\[ SE_B(\hat\alpha) = \sqrt{\frac{1}{B-1}\sum^B_{r=1}\bigg(\hat\alpha^{*r}-\frac{1}{B}\sum^B_{r'=1}\hat\alpha^{*r'}\bigg)^2}  \tag{4} \]</p>

<p>Thus, \[SE_B(\hat\alpha)\] serves as an estimate of the standard error of \[\hat\alpha\] estimated from the original data set.  Let&#39;s look at how we can implement this in R on a couple of simple examples:</p>

<h3>Example 1: Estimating the accuracy of a single statistic</h3>

<p>Performing a bootstrap analysis in R entails two steps:</p>

<ol>
<li>Create a function that computes the statistic of interest.</li>
<li>Use the <code>boot</code> function from the <a href="http://cran.r-project.org/web/packages/boot/index.html"><code>boot</code></a> package to perform the boostrapping</li>
</ol>

<p>In this example we&#39;ll use the <code>ISLR::Portfolio</code> data set.  This data set contains the returns for two investment assets (<em>X</em> and <em>Y</em>). Here, our goal is going to be minimizing the risk of investing a fixed sum of money in each asset.  Mathematically, we can achieve this by minimizing the variance of our investment using the statistic</p>

<p>\[ \hat\alpha = \frac{\hat\sigma^2_Y - \hat\sigma_{XY}}{\hat\sigma^2_X +\hat\sigma^2_Y-2\hat\sigma_{XY}}  \tag{5} \]</p>

<p>Thus, we need to create a function that will compute this test statistic:</p>

<pre><code class="r">statistic &lt;- function(data, index) {
  x &lt;- data$X[index]
  y &lt;- data$Y[index]
  (var(y) - cov(x, y)) / (var(x) + var(y) - 2* cov(x, y))
}
</code></pre>

<p>Now we can compute \[\hat\alpha\] for a specified subset of our portfolio data:</p>

<pre><code class="r">portfolio &lt;- ISLR::Portfolio

# compute our statistic for all 100 observations
statistic(portfolio, 1:100)
## [1] 0.5758321
</code></pre>

<p>Next, we can use <code>sample</code> to randomly select 100 observations from the range 1 to 100, with replacement. This is equivalent to constructing a new bootstrap data set and recomputing \[\hat\alpha\] based on the new data set.  </p>

<pre><code class="r">statistic(portfolio, sample(100, 100, replace = TRUE))
## [1] 0.7661566
</code></pre>

<p>If you re-ran this function several times you&#39;ll see that you are getting a different output each time.  What we want to do is run this <em>many</em> times, record our output each time, and then compute a valid standard error of all the outputs.  To do this we can use <code>boot</code> and supply it our original data, the function that computes the test statistic, and the number of bootstrap replicates (<code>R</code>).</p>

<pre><code class="r">set.seed(123)
boot(portfolio, statistic, R = 1000)
## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = portfolio, statistic = statistic, R = 1000)
## 
## 
## Bootstrap Statistics :
##      original      bias    std. error
## t1* 0.5758321 0.002396754  0.08752118
</code></pre>

<p>The final output shows that using the original data, \[\hat\alpha = 0.5758\], and it also provides the bootstrap estimate of our standard error \[SE(\hat\alpha) = 0.0875\].</p>

<p>Once we generate the bootstrap estimates we can also view the confidence intervals with <code>boot.ci</code> and plot our results:</p>

<pre><code class="r">set.seed(123)
result &lt;- boot(portfolio, statistic, R = 1000)

boot.ci(result, type = &quot;basic&quot;)
## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 1000 bootstrap replicates
## 
## CALL : 
## boot.ci(boot.out = result, type = &quot;basic&quot;)
## 
## Intervals : 
## Level      Basic         
## 95%   ( 0.3958,  0.7376 )  
## Calculations and Intervals on Original Scale
plot(result)
</code></pre>

<p><img src="/public/images/analytics/resampling/unnamed-chunk-19-1.png" style="display: block; margin: auto;" /></p>

<h3>Example 2: Estimating the accuracy of a linear regression model</h3>

<p>We can use this same concept to assess the variability of the coefficient estimates and predictions from a statistical learning method such as linear regression.  For instance, here we&#39;ll assess the variability of the estimates for \[\beta_0\] and \[\beta_1\], the intercept and slope terms for the linear regression model that uses <code>horsepower</code> to predict <code>mpg</code> in our <code>auto</code> data set.  </p>

<p>First, we create the function to compute the statistic of interest. We can apply this to our entire data set to get the baseline coefficients.</p>

<pre><code class="r">statistic &lt;- function(data, index) {
  lm.fit &lt;- lm(mpg ~ horsepower, data = data, subset = index)
  coef(lm.fit)
}

statistic(auto, 1:392)
## (Intercept)  horsepower 
##  39.9358610  -0.1578447
</code></pre>

<p>Now we can inject this into the <code>boot</code> function to compute the bootstrapped standard error estimate:</p>

<pre><code class="r">set.seed(123)
boot(auto, statistic, 1000)
## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = auto, statistic = statistic, R = 1000)
## 
## 
## Bootstrap Statistics :
##       original        bias    std. error
## t1* 39.9358610  0.0295956008 0.863541674
## t2* -0.1578447 -0.0002940364 0.007598619
</code></pre>

<p>This indicates that the bootstrap estimate for \[SE(\beta_0)\] is 0.86, and that the bootstrap estimate for \[SE(\beta_1)\] is 0.0076. If we compare these to the standard errors provided by the <code>summary</code> function we see a difference.  </p>

<pre><code class="r">
summary(lm(mpg ~ horsepower, data = auto))
## 
## Call:
## lm(formula = mpg ~ horsepower, data = auto)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -13.5710  -3.2592  -0.3435   2.7630  16.9240 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 39.935861   0.717499   55.66   &lt;2e-16 ***
## horsepower  -0.157845   0.006446  -24.49   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.906 on 390 degrees of freedom
## Multiple R-squared:  0.6059, Adjusted R-squared:  0.6049 
## F-statistic: 599.7 on 1 and 390 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>This difference suggests the standard errors provided by <code>summary</code> may be biased.  That is, certain assumptions may be violated which is causing the standard errors in the non-bootstrap approach to be different than those in the bootstrap approach.</p>

<p>If you remember from earlier in the tutorial we found that a quadratic fit appeared to be the most approapriate for the relationship between <code>mpg</code> and <code>horsepower</code>.  Lets adjust our code to capture this fit and see if we notice a difference with our outputs.</p>

<pre><code class="r">quad.statistic &lt;- function(data, index) {
  lm.fit &lt;- lm(mpg ~ poly(horsepower, 2), data = data, subset = index)
  coef(lm.fit)
}

set.seed(1)
boot(auto, quad.statistic, 1000)
## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = auto, statistic = quad.statistic, R = 1000)
## 
## 
## Bootstrap Statistics :
##       original      bias    std. error
## t1*   23.44592 0.003943212   0.2255528
## t2* -120.13774 0.117312678   3.7008952
## t3*   44.08953 0.047449584   4.3294215
</code></pre>

<p>Now if we compare the standard errors between the bootstrap approach and the non-bootstrap approach we see the standard errors align more closely.  This better correspondence between the bootstrap estimates and the standard estimates suggest a better model fit.  Thus, bootstrapping provides an additional method for assessing the adequacy of our model&#39;s fit.</p>

<pre><code class="r">summary(lm(mpg ~ poly(horsepower, 2), data = auto))
## 
## Call:
## lm(formula = mpg ~ poly(horsepower, 2), data = auto)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.7135  -2.5943  -0.0859   2.2868  15.8961 
## 
## Coefficients:
##                       Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)            23.4459     0.2209  106.13   &lt;2e-16 ***
## poly(horsepower, 2)1 -120.1377     4.3739  -27.47   &lt;2e-16 ***
## poly(horsepower, 2)2   44.0895     4.3739   10.08   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.374 on 389 degrees of freedom
## Multiple R-squared:  0.6876, Adjusted R-squared:  0.686 
## F-statistic:   428 on 2 and 389 DF,  p-value: &lt; 2.2e-16
</code></pre>

<h2>Additional Resources {#additional}</h2>

<p>This will get you started with resampling methods; however, understand that there are many approaches for resampling and even more options within R to implement these approaches. The following resources will help you learn more:</p>

<ul>
<li><a href="http://www-bcf.usc.edu/%7Egareth/ISL/">An Introduction to Statistical Learning</a></li>
<li><a href="https://www.amazon.com/Introduction-Statistics-Through-Resampling-Methods/dp/1118428218/ref=sr_1_2?ie=UTF8&amp;qid=1490920595&amp;sr=8-2&amp;keywords=resampling+r">Introduction to Statistics Through Resampling Methods and R</a></li>
<li><a href="http://appliedpredictivemodeling.com/">Applied Predictive Modeling</a></li>
<li><a href="https://statweb.stanford.edu/%7Etibs/ElemStatLearn/">Elements of Statistical Learning</a></li>
</ul>

<p>[<sup>islr]:</sup> This tutorial was built as a supplement to chapter 5 of <a href="http://www-bcf.usc.edu/%7Egareth/ISL/">An Introduction to Statistical Learning</a></p>

    
    <footer class = 'logo'>
<div style="position: absolute; left: 900px; top: 600px; z-index:100">
<img src = "assets/img/publichealthlogo.png" height="100" >
</div>
</footer>
  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>
  <div class="pagination pagination-small" id='io2012-ptoc' style="display:none;">
    <ul>
      <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=1 title='Resampling Methods'>
         1
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=2 title='Resampling Methods'>
         2
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=3 title='Resampling Methods'>
         3
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=4 title='Why Resampling {#why}'>
         4
      </a>
    </li>
  </ul>
  </div>  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
  <!-- Load Javascripts for Widgets -->
  <script src="libraries/widgets/quiz/js/jquery.quiz.js"></script>
<script src="libraries/widgets/quiz/js/mustache.min.js"></script>
<script src="libraries/widgets/quiz/js/quiz-app.js"></script>
<script src="libraries/widgets/bootstrap/js/bootstrap.min.js"></script>
<script src="libraries/widgets/bootstrap/js/bootbox.min.js"></script>
<script src="libraries/widgets/interactive/js/ace/js/ace.js"></script>
<script src="libraries/widgets/interactive/js/opencpu-0.5.js"></script>
<script src="libraries/widgets/interactive/js/interactive.js"></script>

  <!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true
      }
    });
  </script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script> -->
  <script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<script>  
  $(function (){ 
    $("#example").popover(); 
    $("[rel='tooltip']").tooltip(); 
  });  
  </script>  
  <!-- LOAD HIGHLIGHTER JS FILES -->
  <script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <!-- DONE LOADING HIGHLIGHTER JS FILES -->
   
  </html>