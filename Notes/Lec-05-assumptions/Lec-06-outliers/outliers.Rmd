---
title: "Outliers and Influential Observations"
author: "Adam J Sullivan, PhD"
date: "02/12/2018"
output:
   ioslides_presentation: 
    widescreen: true
notes: ''
link: yes
slides: yes
layout: page
css:  "C:/Users/adam_/Dropbox (Personal)/Brown/Teaching/Brown Courses/PHP2511/Spring 2018/website/php-1511-2511.github.io/Notes/slides.css"
---

```{r setup, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(error = TRUE)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_chunk$set(results="hold")
knitr::opts_chunk$set(cache=F)
```



## Leverage and Outliers

- We will now move onto leverage points and outliers. 
- **Leverage point**: is a value of the predictor that is far from the average of the predictor variables. 
- **Outlier points**: is a values of the outcome that is far from the average of the outcome. 




## Leverage and Outliers

- These two things together help us determine whether certain points have a lot of influence on our regression model. 
- For example in Anscombe model 3 it appears that there is one point that not only is an outlier but may be a leverage point as well. 
- Instead of trying to parse both of these concepts out we will focus on a plot that helps us consider influential points as a whole. 


## Cook's D 

- Cook's distance attempts to tell us how much $\hat{\beta}$ changes due to the inclusion of the $i^{th}$ observation. 

$$D_i= \dfrac{\sum_{j=1}^n \left( \hat{y}_j - \hat{y}_{j(i)}\right)^2}{(p+1)\hat{\sigma}^2}$$

## DFFITS

- This quantity measures how much the regression function changes at the  $i$-th case / observation when the  $i$-th case / observation is deleted.
- For small/medium datasets: value of 1 or greater is “suspicious” (RABE). For large dataset: value of  $2\sqrt{(p+1)/n}$.

$$DFFITS_i=\dfrac{\hat{Y_i} - \hat{Y_{i(i)}}}{\hat{\sigma}_{(i)}\sqrt{h_{ii}}}$$

## What is $h_{ii}$?

- In regression we have something we call the hat matrix, for a matrix $X$:
$$H=X(X^TX)^{-1}X^T$$
- We actually solve regression by performing this operation:
$$\hat{y} = Hy$$

## What is $h_{ii}$?

- This ends up meaning that we have:
$$\hat{y}_i = h_{i1}y_1 + h_{i2}y_2 + \cdots + h_{ii}y_i + \cdots + h_{in}y_n$$


## DFBETAS

- This quantity measures how much the coefficients change when the $i$-th case is deleted.
$$DFBETAS_{j(i)}= \dfrac{\hat{\beta}_j - \hat{\beta}_{j(i)}}{\sqrt{\hat{\sigma}^2_{(i)}(X^TX)^{-1}_{jj}}}$$
- For small/medium datasets: absolute value of 1 or greater is “suspicious”. For large dataset: absolute value of  $2/\sqrt{n}$ .


## Simulating Data

```{R}
set.seed(12345)
X = runif(150, .5, 3.5)
beta0 = 1.0
beta1 = 1.5
sigma = 0.7
Y = beta0 + beta1*X + sigma*rnorm(150) # The regular process
# Contaminated data: Four cases
X.suspect1 = 1.5; Y.suspect1 = 3.3
X.suspect2 = 1.5; Y.suspect2 = 9.7
X.suspect3 = 9.0; Y.suspect3 = 14.5
X.suspect4 = 9.3; Y.suspect4 = 0.6
Y.all1 = c(Y, Y.suspect1); X.all1 = c(X, X.suspect1)
Y.all2 = c(Y, Y.suspect2); X.all2 = c(X, X.suspect2)
Y.all3 = c(Y, Y.suspect3); X.all3 = c(X, X.suspect3)
Y.all4 = c(Y, Y.suspect4); X.all4 = c(X, X.suspect4)


```


## Plots of Data

```{r, echo=F}
library(tidyverse)
library(gridExtra)
library(ggplot2)
data <- as_data_frame(cbind(X.all1, X.all2, X.all3, X.all4, Y.all1, Y.all2, Y.all3, Y.all4))

p1 <- ggplot(data, aes(X.all1, Y.all1)) + geom_point() +
annotate("point", x = X.suspect1, y = Y.suspect1, colour = "red", size=3) + xlab("X") + ylab("Y")

p2 <- ggplot(data, aes(X.all2, Y.all2)) + geom_point() +
  annotate("point", x = X.suspect2, y = Y.suspect2, colour = "red", size=3)+ xlab("X") + ylab("Y")

p3 <- ggplot(data, aes(X.all3, Y.all3)) + geom_point() +
  annotate("point", x = X.suspect3, y = Y.suspect3, colour = "red", size=3)+ xlab("X") + ylab("Y")

p4 <- ggplot(data, aes(X.all4, Y.all4)) + geom_point() +
  annotate("point", x = X.suspect4, y = Y.suspect4, colour = "red", size=3)+ xlab("X") + ylab("Y")


grid.arrange(p1,p2,p3,p4, ncol=2)

```

## Run the 4 Regressions

```{r}
out1 <- lm(data=data, Y.all1~X.all1 )
out2 <- lm(data=data, Y.all2~X.all2 )
out3 <- lm(data=data, Y.all3~X.all3 )
out4 <- lm(data=data, Y.all4~X.all4 )
```


## Outliers and Influential Points Plots


```{r, eval=F}
library(olsrr)
ols_cooksd_barplot(out1)
ols_dfbetas_panel(out1)
ols_dffits_plot(out1)
```


## Outliers and Influential Points Plots: Cook's D

```{r}
library(olsrr)
ols_cooksd_barplot(out1)
```

## Outliers and Influential Points Plots: DFBETAS

```{r}
library(olsrr)
ols_dfbetas_panel(out1)
```

## Outliers and Influential Points Plots: DFFITS

```{r}
library(olsrr)
ols_dffits_plot(out1)
```




## Outliers and Influential Points Plots


```{r, eval=F}
library(olsrr)
ols_cooksd_barplot(out4)
ols_dfbetas_panel(out4)
ols_dffits_plot(out4)
```

## Outliers and Influential Points Plots: Cook's D

```{r}
library(olsrr)
ols_cooksd_barplot(out4)
```

## Outliers and Influential Points Plots: DFBETAS

```{r}
library(olsrr)
ols_dfbetas_panel(out4)
```

## Outliers and Influential Points Plots: DFFITS

```{r}
library(olsrr)
ols_dffits_plot(out4)
```






## What can we do with this point?

- We can decide to remove the point and re-run the regression. 

```{r, eval=F}
library(broom)
out4a <- lm(data=data[-151,], Y.all4~X.all4 )

tidy4a <- tidy(out4a, conf.int = T)
tidy4 <- tidy(out4, conf.int = T)
knitr::kable(bind_rows(tidy4, tidy4a)[-c(3,4)])

glance4a <- glance(out4a)
glance4 <- glance(out4)
knitr::kable(bind_rows(glance4, glance4a))
```


## What can we do with this point?

```{r, echo=F}
library(broom)
out4a <- lm(data=data[-151,], Y.all4~X.all4 )

tidy4a <- tidy(out4a, conf.int = T)
tidy4 <- tidy(out4, conf.int = T)
knitr::kable(bind_rows(tidy4, tidy4a)[-c(3,4)])

```


## What can we do with this point?

```{r, echo=F}
glance4a <- glance(out4a)
glance4 <- glance(out4)
knitr::kable(bind_rows(glance4, glance4a))
```


## Marginal Model Plots

- We will consider the next level of plots called Marginal Model Plots. 
- The aim of these plots is to show how well out model fits the data. 




```{r, eval=F}
  library(car)
mmps(out4)
```




## Marginal Model Plots

```{r, echo=F}
  library(car)
mmps(out4)
```



## What Can we See?

- From the figure we can see that the blue line represents a loess(smoothing) line for the data and the dashed line represents the model which R fitted. 
- We can see that our data is very skewed by the outlier 
- Also we can see that the loess line is more curved than our data model. 


## What happens when we Delete points?

- When we remove the point the difference is drastic 
```{r, eval=F}
mmps(out4a)
```


## What happens when we Delete Points??


```{r, echo=F}
mmps(out4a)
```

## Outlier Treatment


- Once the outliers are identified and you have decided to make amends as per the nature of the problem, you may consider one of the following approaches.
    1. Imputation
    2. Capping
    3. Prediction
    

## Imputation

We can impute the value by replacing it with:

- mean
- median
- mode
- Other Regression techniques

We will consider this further in missing Data. 


## Capping

- For missing values that lie outside the 1.5*IQR limits, we could cap it by replacing those observations outside the lower limit with the value of 5th %ile and those that lie above the upper limit, with the value of 95th %ile. 
- Below is a sample code that achieves this.
```
x <- dataframe$variable_of)interest
qnt <- quantile(x, probs=c(.25, .75), na.rm = T)
caps <- quantile(x, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(x, na.rm = T)
x[x < (qnt[1] - H)] <- caps[1]
x[x > (qnt[2] + H)] <- caps[2]
```

## Prediction

- In yet another approach, the outliers can be replaced with missing values (NA) and then can be predicted by considering them as a response variable. 
- We will discuss this when considering missing data. 
