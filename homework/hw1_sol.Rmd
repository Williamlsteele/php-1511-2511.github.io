---
title: "Homework 1"
author: "Your Name"
date: "February 7, 2018 at 11:59pm"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(error = TRUE)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_chunk$set(results="hold")
knitr::opts_chunk$set(cache=TRUE)
knitr::opts_chunk$set(echo=FALSE)
```

### Homework Policies:

*You are encouraged to discuss problem sets with your fellow students (and with the Course Instructor of course), but you must write your own final answers, in your own words. Solutions prepared ``in committee'' or by copying someone else's paper are not acceptable.  This violates the Brown standards of plagiarism, and you will not have the benefit of having thought about and worked the problem when you take the examinations.*

*All answers must be in complete sentences and all graphs must be properly labeled.*

***For the PDF Version of this assignment: [PDF](https://raw.githubusercontent.com/php-1511-2511/php-1511-2511.github.io/master/homework/php2511_hw1.pdf)***

***For the R Markdown Version of this assignment: [RMarkdown](https://raw.githubusercontent.com/php-1511-2511/php-1511-2511.github.io/master/homework/php2511_hw1.Rmd)***

### Turning the Homework in:

*Please turn the homework in through canvas. You may use a pdf, html or word doc file to turn the assignment in.*

[PHP 1511 Assignment Link](https://canvas.brown.edu/courses/1075279/assignments/7688355)

[PHP 2511 Assignment Link](https://canvas.brown.edu/courses/1075280/assignments/7688356)

## The Data

This homework will use the following data:


* `hw1a` - https://raw.githubusercontent.com/php-1511-2511/php-1511-2511.github.io/master/Data/hw1a.csv
* `hw1b` - https://raw.githubusercontent.com/php-1511-2511/php-1511-2511.github.io/master/Data/hw1b.csv

##Part 1

1. The data set hw1a is simulated from a famous example that illustrates how variables in multiple regression can be used to predict the response variable, here `Y`, jointly even though they do not necessarily predict `Y` very well individually. There are two predictor variables in the data set, `X1` and `X2`.
  a. Open the data set and begin by looking at all possible two-way scatterplots. Comment on the relationships that you observe.
```{r}
hw1a <- read.csv("https://raw.githubusercontent.com/php-1511-2511/php-1511-2511.github.io/master/Data/hw1a.csv")

library(ggplot2)
library(gridExtra)

p1 <- ggplot(hw1a, aes(x1,y)) + geom_point()
p2 <- ggplot(hw1a, aes(x2,y)) + geom_point()
p3 <- ggplot(hw1a, aes(x1,x2)) + geom_point()

grid.arrange(p1,p2,p3,ncol=2)
```

<br>
<font color="blue">
<b> We can see that it is hard to tell if there is a relationship between `X1` and `Y`. However there does appear to be a positive linear relationship between `X2` and $Y$. We can also see a strong relationship between `X1` and `X2`. </b>
</font>
    
    
  b. Next, examine the simple linear regressions of each predictor to explain `Y`. Comment on whether the predictors seem to relate to `Y`. What percent of the variability in Y does each predictor explain by itself?

```{r}
library(broom)
library(dplyr)
fit.x1 <- lm (y~x1, data=hw1a)
fit.x2 <- lm(y~x2, data=hw1a)
tidy1 <- tidy(fit.x1, conf.int=TRUE)[,-c(3:4)]
tidy2 <- tidy(fit.x2, conf.int=TRUE)[,-c(3:4)]
knitr::kable(bind_rows(tidy1,tidy2))

glance1 <- glance(fit.x1)[,c(1:5)]
glance2 <- glance(fit.x2)[,c(1:5)]
knitr::kable(bind_rows(glance1,glance2))
```

<br>
<font color="blue">
<b>We can see in `fit.x1` that `X1` does not have a significant relationship with `Y`. The model explains <1% of the variation in `Y`. For `fit.x2` we can see that it has a significant positive relationship with `Y`. This model explains 19.3% of the variation in `Y`.</b>
</font>
  
  c. Now use `lm()` to build a multiple regression model using both predictor variables `X1` and `X2`. Comment on the fit and the statistical significance of each predictor variable. What percent of the variability in Y is explained by the model now that both predictors are included? Give an explanation for what you think is happening with both predictors in the model.
  
  
```{r}
fit.x1x2 <- lm(y~x1+x2, hw1a)
tidy3 <- tidy(fit.x1x2, conf.int=TRUE)[,-c(3:4)]
glance3 <- glance(fit.x1x2)[,c(1:5)]
knitr::kable(tidy3)
knitr::kable(glance3)
```

<font color="blue">
<b>We can see that when we include `X1` and `X2` in the model together that `x2` remains significant but has an increased estimated effect given a level of `X2`. This time `X1` has a significant effect and it shows to be negative given a level of `X2`. Togteher they explain 97.94% of the variation in `Y`. It is hard to say what exactly is going on without investigating further. It seems that when controlling for both `X1` and ` X2` at the same time we are able to account  for more variation in `Y`.   </b>
</font>

  d. Comment on the change in the estimated coefficients from the simple linear regression models compared to those from the multiple regression model. Are the changes qualitative (direction), quantitative (magnitude) or both?
  
<font color="blue">
<b>With `X1` there is a change in both qualitative (direction) and quantitative (magnitude). With `X2` there is only a  quantitative (magnitude) change.</b>
</font>
  
  e. Run the following code to create a 3d scatterplot. Notice that multiple linear regression is now a plane and not just a line. Why do you think `X1` and `X2` predict `Y` so well together when they do not alone? 


```{R, message=F, warning=F}
library(plotly)
hw1a <- read.csv("https://raw.githubusercontent.com/php-1511-2511/php-1511-2511.github.io/master/Data/hw1a.csv")

p <- plot_ly(hw1a, x = ~x1, y = ~x2, z = ~y) %>%
  add_markers() %>%
  layout(scene = list(xaxis = list(title = 'x1'),
                     yaxis = list(title = 'x2'),
                     zaxis = list(title = 'y')))

p
```

<font color="blue">
<b>We can see that the relationship between all of these is actually a plane. This suggests that it requires more to explain the variability in `Y` than just one variable alone. Everything we have seen shows that when you have the same level of `X1` that an increase in `X2` will lead to an increase in `Y`. However when you have the same level of `X2` an increase in `X1` leads to a decrease in `Y`. This partitioning of the data allows us greater understanding of the variance of `Y`.  </b>
</font>

---

## Part 2 

### The Data

Data set `hw1b` contains air pollution data from 41 U.S. cities. Our goal is to try to build a multiple regression model to predict SO2 concentration
using the other variables. 


Variable Name  | Description
--------------- | ----------
`so2` |SO2 air concentration in micrograms per cubic meter.
`temp` | Average Annual temperature in degrees F.
`empl20` | The number of manufacturing companies with 20 or more workers.
`pop` | The population in thousands.
`wind` | The average annual wind speeds in miles per hour. 
`precipin` | The average annual precipitation in inches. 
`precipdays` | The average number of days with precipitation per year. 

---

2. Load data set HW1b and answer the following questions. *Display all useful code and output inline. Do not just display all that R gives you but display parts that show why you chose the model you did.* 
  a. Begin by examining univariate summaries of the 7 variables. Do any of the points seem to have extreme values? Comment on whether cities with extreme values also have extremes on one or more other variables.

```{r, results=FALSE}
hw1b <- read.csv("https://raw.githubusercontent.com/php-1511-2511/php-1511-2511.github.io/master/Data/hw1b.csv")
```
```{r, eval=FALSE}
library(pander)
pandoc.table(rbind(summary(hw1b[,1]), summary(hw1b[,2]), summary(hw1b[,3]), summary(hw1b[,4]), summary(hw1b[,5]), summary(hw1b[,6]), summary(hw1b[,7])))
```



<font color="blue">
  
----------------------------------------------------------------------------------
Variable    Minimum   1st Qua.  Median   Mean       3rd. Qua.  Maximum
---------- ---------- --------- -------- ---------- ---------- -------------------
  SO2          8       13        26       30.05         35      110 
  
  temp        43.5    50.6      54.6      55.76         59.3    75.5 
  
  empl20      35      181       347       463.1         462     3344 
  
  pop         71      299       515       608.6         717     3369
  
  wind        6       8.7       9.3       9.444         10.6     12.7 
  
 Precipin     7.05    30.96     38.74     36.77         43.11    59.8 
 
 preicdays    36      103       115       113.9         128       166
----------------------------------------------------------------------------------

<b>We can see that the table above shows that the number of manufacturing companies with 20 or more workers seems to have some extreme values. Since 75% of the data falls at 462 and below, however the max is 3344. Population also seems like it may have some extreme values since 75% of the values are at or below 717 but the maximum is at 3369. We will evaluate these 2 futher with boxplots.</b>
</font>

```{r}
library(car)
par(mfrow=c(1,2), bg="ivory")
Boxplot(hw1b$empl20)
Boxplot(hw1b$pop)
```
    
<font color="blue">
<b>From the above graphs we can see that there do appear to be some extreme values with population and number of companies with 20 or more eomployees. Record 11 seems to the the largest of these values.</b>

</font>
  b. Start your model building by looking at simple linear regressions for each of the 6 predictor variables. Display and Examine relevant plots. Summarize the simple linear regression results using the broom package. Note that you can combine tidy statements:
  
<font color="blue">
```{r}
mod1 <- lm(so2 ~ temp, hw1b)
mod2 <- lm(so2 ~ empl20, hw1b)
mod3 <- lm(so2 ~ pop, hw1b)
mod4 <- lm(so2 ~ wind, hw1b)
mod5 <- lm(so2 ~ precipin, hw1b)
mod6 <- lm(so2 ~ precipdays, hw1b)


tidy1 <- tidy(mod1, conf.int=TRUE)[-1,-c(3:4)]
tidy2 <- tidy(mod2, conf.int=TRUE)[-1,-c(3:4)]
tidy3 <- tidy(mod3, conf.int=TRUE)[-1,-c(3:4)]
tidy4 <- tidy(mod4, conf.int=TRUE)[-1,-c(3:4)]
tidy5 <- tidy(mod5, conf.int=TRUE)[-1,-c(3:4)]
tidy6 <- tidy(mod6, conf.int=TRUE)[-1,-c(3:4)]

knitr::kable(bind_rows(tidy1,tidy2, tidy3,tidy4,tidy5,tidy6))



glance1 <- glance(mod1)[,c(1:5)]
glance2 <- glance(mod2)[,c(1:5)]
glance3 <- glance(mod3)[,c(1:5)]
glance4 <- glance(mod4)[,c(1:5)]
glance5 <- glance(mod5)[,c(1:5)]
glance6 <- glance(mod6)[,c(1:5)]
knitr::kable(bind_rows(glance1, glance2, glance3, glance4, glance5, glance6))
```

<br>

<b>We can see that with temperature we do have a significant negative predicted value which explains about 19% of the variation in So2 levels. With the number of companies with 20+ employees we have a positive highly significant relationship that explains about 42% of the variation in So2 levels. With population we see that we have a significant positive relationship that explains about 24% of the variation in So2 levels. With Average annual wind we have an insignificant relationship and it explains about 1% of the variation. With Average annual precipitation we have an insignificant relationship and it explains <1% of the variation. With Average annual days of precipitation we have an significant positive relationship and it explains about 14% of the variation.</b>


</font>

  c. Build a multiple regression model by sequentially adding variables that you feel are important from the simple linear regressions.
  
<font color="blue">

<br>From our discussion above we will begin with the 2 variables with the best fit. Which is companies with 20+ employees and population.<b>
```{r}
mod7 <- lm(so2 ~ empl20 + pop, hw1b)
tidy7 <- tidy(mod7, conf.int=TRUE)[,-c(3:4)]
glance7 <- glance(mod7)[,c(1:5)]
knitr::kable(tidy7)
knitr::kable(glance7)
```

<b>We can see from the model summary that both `empl20` and `pop` have significant effects on `So2`. `empl120` shows a positive relationship given `pop` which is actually larger than it was before adjusting for `pop`. When adjusting for `empl20` the population actually has a negative relationship with `so2`. From here I will try and add in temperature. </b>

```{r}
mod8 <- lm(so2 ~ empl20 + pop +temp, hw1b)
tidy8 <- tidy(mod8, conf.int=TRUE)[,-c(3:4)]
glance8 <- glance(mod8)[,c(1:5)]
knitr::kable(tidy8)
knitr::kable(glance8)
```


<b> With the addition of temperaure. My adjusted $R^2$ increased only a small amount to 0.6125. This would suggest that temperature does add something. </b>

</font>    
<font color="red">

<b>I proceeded checking different models in this fashion. Below I will discuss the final model I chose. </b></font> 



d. State your final multiple regression model, interpret the parameter estimates and the R2. Comment on any differences in coefficients between the simple linear regression models and the multiple regression model.

<font color="blue">
```{r}
mod9 <- lm(so2 ~ empl20 + pop + precipin + wind + temp, hw1b)
tidy9 <- tidy(mod9, conf.int=TRUE)[,-c(3:4)]
glance9 <- glance(mod9)[,c(1:5)]
knitr::kable(tidy9)
knitr::kable(glance9)
```

Your answer may differ from mine but here is my reasoning for why I chose my model. <b>I chose to go back to my model with the predictors of `pop` ,`empl20`, `precipin`, `wind` and `temp`. This model has an overall $R^2$ of 0.6685. So it explains about 67% of the variation. If you compare this model to other subsets you will find that it also have the largest adjusted $R^2$ out of them. We can see that `population` and `wind` changes both in magnidute and in direction of effect. Even though in this model `precipin` and `wind` are not significant, the model allows me to explain about 6% more variation than if I left them out. </b>
</font>

  e. Complete the following with the final model you built in part d. 
    i. What does the adjusted $R^2$ tell you about your model fit?
    <font color="blue">
   <b> Adjusted $R^2$ is a way for us to compare models. It is a way of weighting the $R^2$ to show the effect of adding extra variables.  </b>
    </font>
    ii. Perform a hypothesis test on the slope estimates for each variable in the model. 
    <font color="blue">
   <b> If we look at the p-values in the table  above we can see that `empl20` is significant with a p-value of 0.0002. We can also see that `pop` is signifianct with a a p-value of 0.012. Finally, `temp` is significant with a p-value of 0.011. The rest have p-values over 0.05 </b>
    </font>

